Here you will find 4 notebooks.

Once we ran the data collection script (spotify_extractor.py) on the 
'dataset-of-10s'CSV to create New_Spotify.csv, we used descriptive 
analytics and cleaning to preprocess most of the data 
(remove outliers, apply log transforms and get descriptive charts)
to understand our data better.

We then ran three sets of models, the baseline book (PreTuning), one that 
utilized GridSearchCV for hyperparameters, and one that utilized 
RandomSearchCV for hyperparameters. 

You can run any of these three notebooks once you have the manipulated data
output from the cleaning process (Manipulated_data.csv)- it already comes 
in this repository so you do not need to preprocess to play with our models.
